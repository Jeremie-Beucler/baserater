[{"path":"https://jeremie-beucler.github.io/baserater/articles/Introduction-to-the-baserater-package.html","id":"downloading-the-data","dir":"Articles","previous_headings":"","what":"1. Downloading the Data","title":"Introduction to the baserater package","text":"can begin downloading article datasets using download_data(). Use download_data() retrieve either: - full base-rate item database (GPT-4 LLaMA 3.3); - validation ratings, include human typicality scores generated GPT-4 LLaMA 3.3 100 group–adjective pairs. - two typicality matrices (GPT-4 LLaMA 3.3) used generate base-rate item database.","code":"# Load the base-rate database database <- download_data(\"database\")  # Load the typicality validation ratings ratings <- download_data(\"validation_ratings\")  # Load the typicality matrices gpt4_matrix <- download_data(\"similarity_matrix_gpt4\") llama3_3_matrix <- download_data(\"similarity_matrix_llama3.3\")"},{"path":"https://jeremie-beucler.github.io/baserater/articles/Introduction-to-the-baserater-package.html","id":"generating-new-typicality-ratings-from-llms-experimental","dir":"Articles","previous_headings":"","what":"2. Generating New Typicality Ratings from LLMs (Experimental)","title":"Introduction to the baserater package","text":"can generate new typicality ratings using Hugging Face-hosted language model via hf_typicality(). Note experimental feature, depends API availability, model compatibility. function works sending structured prompts large language model parsing numeric outputs (0 100) reflect well description (e.g., adjective) fits given group. default, function uses prompt generation parameters described paper. ensure reproducibility, include precomputed result obtained using model Llama-3.1-8B-Instruct, smaller older version Llama-3.3-70B-Instruct available Hugging Face: ’d like generate new scores , ’ll need: - valid Hugging Face API token (use Sys.setenv(HF_API_TOKEN = \"your_token\") pass directly); - accept terms use model Hugging Face (necessary); - ensure prompt matches formatting expectations model ’re using (e.g., special tags, instruction tokens, etc.). example function call might look: hf_typicality() function supports two modes: - matrix = TRUE (default): Computes cross-product unique groups descriptions. Returns list matrices scores responses. - matrix = FALSE: Computes row--row scores group–adjective pairs supply. Returns tibble. See ?hf_typicality full documentation customization options.","code":"# Load pre-generated scores new_scores <- readRDS(system.file(\"extdata\", \"new_typicality_scores_llama3.1_8B.rds\", package = \"baserater\"))  head(new_scores) #> # A tibble: 6 × 5 #>   group               description mean_score raw_scores full_responses #>   <chr>               <chr>            <dbl> <list>     <list>         #> 1 DJ                  naive               56 <dbl [3]>  <chr [3]>      #> 2 accountant          solitary            58 <dbl [3]>  <chr [3]>      #> 3 actor               altruistic          74 <dbl [3]>  <chr [3]>      #> 4 aerobics instructor impulsive           42 <dbl [2]>  <chr [3]>      #> 5 architect           charismatic         22 <dbl [3]>  <chr [3]>      #> 6 architect           reserved            85 <dbl [3]>  <chr [3]> # # Original prompt from the paper # original_system_prompt_content <- \"You are expert at accurately reproducing the stereotypical associations humans make, in order to annotate data for experiments. Your focus is to capture common societal perceptions and stereotypes, rather than factual attributes of the groups, even when they are negative or unfounded.\" #  # original_user_prompt_content_template <- 'Rate how well the adjective \"{description}\" reflects the prototypical member of the group \"{group}\" on a scale from 0 (\"Not at all\") to 100 (\"Extremely\"). #  # To clarify, consider the following examples: #  # 1. \"Rate how well the adjective \"FUNNY\" reflects the prototypical member of the group \"CLOWN\" on a scale from 0 (Not at all) to 100 (Extremely).\" A high rating is expected because the adjective \"FUNNY\" closely aligns with the typical characteristics of a \"CLOWN\". #  # 2. \"Rate how well the adjective \"FEARFUL\" reflects the prototypical member of the group \"FIREFIGHTER\" on a scale from 0 (Not at all) to 100 (Extremely).\" A low rating is expected because the adjective \"FEARFUL\" diverges significantly from the typical characteristics of a \"FIREFIGHTER\". #  # 3. \"Rate how well the adjective \"PATIENT\" reflects the prototypical member of the group \"ENGINEER\" on a scale from 0 (Not at all) to 100 (Extremely).\" A mid-scale rating is expected because the adjective \"PATIENT\" neither closely aligns nor diverges significantly from the typical characteristics of an \"ENGINEER\". #  # Your response should be a single score between 0 and 100, with no additional text, letters, or symbols included.' #  # # Llama 3 Formatted Prompts # # see https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/ # # These special tags are required for models in the LLaMA 3 family # llama3_system_prompt <- paste0( #   \"<|begin_of_text|>\", #   \"<|start_header_id|>system<|end_header_id|>\\n\\n\", #   original_system_prompt_content, #   \"\\n<|eot_id|>\" # ) #  # llama3_user_prompt_template <- paste0( #   \"<|start_header_id|>user<|end_header_id|>\\n\\n\", #   original_user_prompt_content_template, #   \"\\n<|eot_id|>\", #   \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\" # )  # # Example using the validation ratings # groups <- ratings$group # descriptions <- ratings$adjective #  # hf_token = \"YOUR TOKEN HERE\" # default = Sys.getenv(\"HF_API_TOKEN\") #  # new_scores <- hf_typicality( #   groups                = groups, #   descriptions          = descriptions, #   model                 = \"meta-llama/Llama-3.1-8B-Instruct\", #   hf_token              = hf_token, #   system_prompt         = llama3_system_prompt, #   user_prompt_template  = llama3_user_prompt_template, #   n                     = 3, #   min_valid             = 2, #   max_tokens            = 3, #   retries               = 1, #   matrix                = FALSE, #   return_raw_scores     = TRUE, #   return_full_responses = TRUE, #   verbose               = TRUE # )"},{"path":"https://jeremie-beucler.github.io/baserater/articles/Introduction-to-the-baserater-package.html","id":"evaluating-new-ratings","dir":"Articles","previous_headings":"","what":"3. Evaluating New Ratings","title":"Introduction to the baserater package","text":"can assess well new model scoring method captures group–adjective typicality comparing ratings human ground truth benchmark models (GPT-4 LLaMA 3.3). Suppose create typicality ratings 100 validation items store data frame three columns: group, adjective, rating. Use evaluate_external_ratings() compute correlations display comparisons: can see, smaller older Llama-3.1-8B-Instruct perform baseline models (GPT-4 LLaMA 3.3-70B).","code":"# Create a data frame with the same structure as the validation set new_scores = new_scores %>%    mutate(adjective = description,          rating = mean_score) %>%   select(group, adjective, rating)  head(new_scores) #> # A tibble: 6 × 3 #>   group               adjective   rating #>   <chr>               <chr>        <dbl> #> 1 DJ                  naive           56 #> 2 accountant          solitary        58 #> 3 actor               altruistic      74 #> 4 aerobics instructor impulsive       42 #> 5 architect           charismatic     22 #> 6 architect           reserved        85 # Print correlation summary with human ground truth and baselines evaluate_external_ratings(new_scores)  # Optionally store the output in a variable results <- evaluate_external_ratings(new_scores) print(results) #> # A tibble: 3 × 3 #>   model                  r        p #>   <chr>              <dbl>    <dbl> #> 1 external           0.505 8.47e- 8 #> 2 mean_gpt4_rating   0.885 2.83e-34 #> 3 mean_llama3_rating 0.818 2.85e-25"},{"path":"https://jeremie-beucler.github.io/baserater/articles/Introduction-to-the-baserater-package.html","id":"constructing-base-rate-items-from-typicality-matrices","dir":"Articles","previous_headings":"","what":"4. Constructing Base-Rate Items from Typicality Matrices","title":"Introduction to the baserater package","text":"compute stereotype strength using extract_base_rate_items(), ’ll need typicality matrix—table scores rows correspond groups columns correspond descriptions (e.g., adjectives). cell represents typical description group. example, can load GPT-4 typicality matrix using download_data() (available LLaMA 3.3): Extract base-rate items applying function: can explore filter output, e.g., view strongest stereotypes:","code":"#' The typicality matrix from GPT-4 is a data frame with group–adjective pairs and their typicality scores head(gpt4_matrix) #> # A tibble: 6 × 67 #>   X      intelligent arrogant nerdy  kind  loud careful argumentative persuasive #>   <chr>        <dbl>    <dbl> <dbl> <dbl> <dbl>   <dbl>         <dbl>      <dbl> #> 1 farmer        55       15.2  11.6  75.3  13.8    84.0          21.2       25.6 #> 2 compu…        90.3     57.8  85.2  42.2  13.4    84.4          30.2       33.4 #> 3 fligh…        50.2     27.7  11.4  84    21.2    83.9          20.7       39   #> 4 high …        63.6     64.2  14.4  64.4  75.2    74.7          64.2       83.4 #> 5 denti…        85.9     39.9  23.8  60.8  10.4    90.6          18.8       53.6 #> 6 lawyer        85.2     74.3  33    33.8  39.6    83.4          83.8       88.6 #> # ℹ 58 more variables: immature <dbl>, active <dbl>, funny <dbl>, #> #   disorganized <dbl>, dishonest <dbl>, gentle <dbl>, sensitive <dbl>, #> #   creative <dbl>, helpful <dbl>, strong <dbl>, brave <dbl>, bossy <dbl>, #> #   unconventional <dbl>, quiet <dbl>, organized <dbl>, reliable <dbl>, #> #   ambitious <dbl>, charming <dbl>, confident <dbl>, efficient <dbl>, #> #   friendly <dbl>, generous <dbl>, naive <dbl>, witty <dbl>, empathetic <dbl>, #> #   stubborn <dbl>, trustworthy <dbl>, meticulous <dbl>, inventive <dbl>, … #' Extract base-rate items from the typicality matrix base_rate_items <- extract_base_rate_items(gpt4_matrix) # View top base-rate items by stereotype strength base_rate_items %>%   arrange(desc(StereotypeStrength)) %>%   head(10) #>                 Group1          Group2 Description Score1 Score2 #> 1     fashion designer       paramedic extravagant  82.80    8.0 #> 2        fashion model       paramedic extravagant  79.60    8.0 #> 3       music producer       paramedic extravagant  78.04    8.0 #> 4                hippy fitness trainer        lazy  72.00    7.4 #> 5              drummer       librarian        loud  83.60    8.6 #> 6           politician fitness trainer        lazy  71.44    7.4 #> 7           politician       paramedic extravagant  76.00    8.0 #> 8                   DJ       librarian        loud  81.20    8.6 #> 9  aerobics instructor       librarian        loud  80.60    8.6 #> 10          politician       paramedic     selfish  83.64    9.0 #>    StereotypeStrength #> 1            2.336987 #> 2            2.297573 #> 3            2.277780 #> 4            2.275186 #> 5            2.274281 #> 6            2.267378 #> 7            2.251292 #> 8            2.245153 #> 9            2.237736 #> 10           2.229297"},{"path":"https://jeremie-beucler.github.io/baserater/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jeremie Beucler. Author, maintainer.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Beucler J (2025). baserater: Tools Base-Rate Item Evaluation LLM Typicality Scoring. R package version 0.0.0.9000, https://jeremie-beucler.github.io/baserater/.","code":"@Manual{,   title = {baserater: Tools for Base-Rate Item Evaluation and LLM Typicality Scoring},   author = {Jeremie Beucler},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://jeremie-beucler.github.io/baserater/}, }"},{"path":"https://jeremie-beucler.github.io/baserater/reference/download_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Load base‑rate database or validation ratings from package — download_data","title":"Load base‑rate database or validation ratings from package — download_data","text":"\"database\" contains base-rate items stereotype strengths GPT-4 LLaMA 3.3.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/download_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load base‑rate database or validation ratings from package — download_data","text":"","code":"download_data(   which = c(\"database\", \"validation_ratings\", \"similarity_matrix_gpt4\",     \"similarity_matrix_llama3.3\"),   dest = NULL )"},{"path":"https://jeremie-beucler.github.io/baserater/reference/download_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load base‑rate database or validation ratings from package — download_data","text":"One \"database\", \"validation_ratings\", \"similarity_matrix_gpt4\", \"similarity_matrix_llama3.3\". dest Optional path copy file (returns data either way).","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/download_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load base‑rate database or validation ratings from package — download_data","text":"tibble requested data.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/download_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load base‑rate database or validation ratings from package — download_data","text":"\"validation_ratings\" contains average typicality ratings 50 participants 100 group–adjective pairs, well ratings GPT-4 LLaMA 3.3. \"similarity_matrix_gpt4\" \"similarity_matrix_llama3.3\" raw typicality matrices generated respective model.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/download_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load base‑rate database or validation ratings from package — download_data","text":"","code":"# Load the full base-rate database (LLM-generated stereotype strengths) database <- download_data(\"database\")  # Load validation ratings (human, GPT-4, LLaMA 3.3) for 100 items ratings <- download_data(\"validation_ratings\")  # Load the raw typicality matrix from GPT-4 gpt4_matrix <- download_data(\"similarity_matrix_gpt4\")  # Load the raw typicality matrix from LLaMA 3.3 llama3_matrix <- download_data(\"similarity_matrix_llama3.3\") #> New names: #> • `` -> `...1`"},{"path":"https://jeremie-beucler.github.io/baserater/reference/evaluate_external_ratings.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate external typicality ratings against human and LLM model baselines — evaluate_external_ratings","title":"Evaluate external typicality ratings against human and LLM model baselines — evaluate_external_ratings","text":"function compares external typicality ratings (e.g., generated new model) validation dataset included baserater. validation set contains average typicality ratings collected 50 Prolific participants subset 100 group–adjective pairs, described accompanying paper. input ratings merged reference set, : Computes correlation (cor.test) external ratings human average; Compares one built-model baselines (default: GPT-4 LLaMA 3.3); Prints clear summary correlation coefficients flags whether external model outperforms baseline; Returns tidy result invisibly.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/evaluate_external_ratings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate external typicality ratings against human and LLM model baselines — evaluate_external_ratings","text":"","code":"evaluate_external_ratings(   df,   method = \"pearson\",   baselines = c(\"mean_gpt4_rating\", \"mean_llama3_rating\") )"},{"path":"https://jeremie-beucler.github.io/baserater/reference/evaluate_external_ratings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate external typicality ratings against human and LLM model baselines — evaluate_external_ratings","text":"df data frame columns adjective, group, rating. Must contain typicality scores 100 validation items used original study. method correlation method use stats::cor.test(). Must one : \"pearson\" (default), \"spearman\", \"kendall\". baselines Character vector column names validation set compare (default: c(\"mean_gpt4_rating\", \"mean_llama3_rating\")).","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/evaluate_external_ratings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate external typicality ratings against human and LLM model baselines — evaluate_external_ratings","text":"tibble (invisibly) one row per model (external baseline), columns model, r, p correlation coefficient p-value.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/evaluate_external_ratings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate external typicality ratings against human and LLM model baselines — evaluate_external_ratings","text":"","code":"if (FALSE) { # \\dontrun{ new_scores <- tibble::tibble(   group = ratings$group,   adjective = ratings$adjective,   rating = runif(100)  # Replace with model predictions ) evaluate_external_ratings(new_scores) } # }"},{"path":"https://jeremie-beucler.github.io/baserater/reference/extract_base_rate_items.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Base-Rate Items from a Typicality Matrix — extract_base_rate_items","title":"Extract Base-Rate Items from a Typicality Matrix — extract_base_rate_items","text":"function processes typicality matrix identify base-rate items comparing typicality scores descriptions unique pairs groups.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/extract_base_rate_items.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Base-Rate Items from a Typicality Matrix — extract_base_rate_items","text":"","code":"extract_base_rate_items(typicality_matrix)"},{"path":"https://jeremie-beucler.github.io/baserater/reference/extract_base_rate_items.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Base-Rate Items from a Typicality Matrix — extract_base_rate_items","text":"typicality_matrix numeric matrix data frame rows groups columns descriptions. data frame, first column assumed contain group names.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/extract_base_rate_items.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Base-Rate Items from a Typicality Matrix — extract_base_rate_items","text":"data frame following columns: Group1 group higher typicality score description. Group2 group lower typicality score. Description description (e.g., adjective) compared. Score1 typicality score Group1. Score2 typicality score Group2. StereotypeStrength log-ratio: log(Score1 / Score2). Always >= 0.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/extract_base_rate_items.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Base-Rate Items from a Typicality Matrix — extract_base_rate_items","text":"pair groups description (e.g., adjective), identifies group received higher typicality score. output includes names groups, scores, log-ratio higher lower score. construction, returned Group1 always higher equal typicality score Group2 given description. ensures resulting StereotypeStrength (defined log(Score1 / Score2)) always positive zero, represents strength stereotypical association favor Group1.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/extract_base_rate_items.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Base-Rate Items from a Typicality Matrix — extract_base_rate_items","text":"","code":"mat <- matrix(runif(9, 1, 100), nrow = 3,               dimnames = list(c(\"GroupA\", \"GroupB\", \"GroupC\"),                               c(\"smart\", \"brave\", \"greedy\"))) extract_base_rate_items(mat) #>   Group1 Group2 Description   Score1    Score2 StereotypeStrength #> 1 GroupB GroupA       smart 83.59897  8.994264          2.2294442 #> 2 GroupA GroupB       brave 16.56364  1.732545          2.2576184 #> 3 GroupA GroupB      greedy 50.27996 29.686957          0.5268988 #> 4 GroupC GroupA       smart 60.47533  8.994264          1.9056485 #> 5 GroupC GroupA       brave 47.17296 16.563636          1.0466111 #> 6 GroupC GroupA      greedy 73.55532 50.279961          0.3804311 #> 7 GroupB GroupC       smart 83.59897 60.475328          0.3237957 #> 8 GroupC GroupB       brave 47.17296  1.732545          3.3042295 #> 9 GroupC GroupB      greedy 73.55532 29.686957          0.9073299"},{"path":"https://jeremie-beucler.github.io/baserater/reference/hf_typicality.html","id":null,"dir":"Reference","previous_headings":"","what":"Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality","title":"Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality","text":"function uses Hugging Face Inference API (compatible endpoint) generate typicality ratings querying large language model (LLM). Important: function work : valid Hugging Face API token (via hf_token HF_API_TOKEN environment variable); valid Hugging Face access token accepted model’s license Hub; specified model available accessible via Hugging Face API hosted inference endpoint; model supports free-text input generates numeric outputs response structured prompts. Calls API rate-limited, may incur usage costs, require internet connection. feature experimental guaranteed work Hugging Face-hosted models.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/hf_typicality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality","text":"","code":"hf_typicality(   groups,   descriptions,   model = \"meta-llama/Llama-3-70B-Instruct\",   custom_url = NULL,   hf_token = Sys.getenv(\"HF_API_TOKEN\"),   n = 25,   min_valid = ceiling(0.8 * n),   temperature = 1,   top_p = 1,   max_tokens = 3,   retries = 4,   matrix = TRUE,   return_raw_scores = TRUE,   return_full_responses = FALSE,   verbose = interactive(),   system_prompt =     \"You are expert at accurately reproducing the stereotypical associations humans make, in order to annotate data for experiments.\\nYour focus is to capture common societal perceptions and stereotypes, rather than factual attributes of the groups, even when they are negative or unfounded.\",   user_prompt_template =     \"Rate how well the description \\\"{description}\\\" reflects the prototypical member of the group \\\"{group}\\\" on a scale from 0 (\\\"Not at all\\\") to 100 (\\\"Extremely\\\").\\n\\nTo clarify, consider the following examples:\\n1. \\\"Rate how well the description \\\"FUNNY\\\" reflects the prototypical member of the group \\\"CLOWN\\\" on a scale from 0 (Not at all) to 100 (Extremely).\\\" A high rating is expected because \\\"FUNNY\\\" closely aligns with typical characteristics of a \\\"CLOWN\\\".\\n2. \\\"Rate how well the description \\\"FEARFUL\\\" reflects the prototypical member of the group \\\"FIREFIGHTER\\\" on a scale from 0 (Not at all) to 100 (Extremely).\\\" A low rating is expected because \\\"FEARFUL\\\" diverges from typical characteristics of a \\\"FIREFIGHTER\\\".\\n3. \\\"Rate how well the description \\\"PATIENT\\\" reflects the prototypical member of the group \\\"ENGINEER\\\" on a scale from 0 (Not at all) to 100 (Extremely).\\\" A mid-scale rating is expected because \\\"PATIENT\\\" neither strongly aligns with nor diverges from typical characteristics of an \\\"ENGINEER\\\".\\n\\nYour response should be a single score between 0 and 100, with no additional text, letters, or symbols.\" )"},{"path":"https://jeremie-beucler.github.io/baserater/reference/hf_typicality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality","text":"groups, descriptions Character vectors. matrix = FALSE must length. model Model ID Hugging Face (ignored custom_url supplied). custom_url Fully-qualified HTTPS URL private Inference Endpoint self-hosted TGI server. hf_token API token (defaults Sys.getenv(\"HF_API_TOKEN\")). n Samples requested per retry block (>= 1). min_valid Minimum numeric scores required per pair. temperature, top_p, max_tokens Generation controls. retries Maximum number additional retry blocks. matrix TRUE = cross-product, FALSE = paired. return_raw_scores TRUE, also returns vector(s) raw valid numeric scores. return_full_responses TRUE, also returns raw text model outputs (error strings failed attempts) query. verbose TRUE, prints progress: pair labels, retry counts, running tallies, raw model responses/errors occur. system_prompt Prompt string system message. See 'Prompting Details' section function signature default content customization. user_prompt_template Prompt template user message, {group} {description} placeholders. prompt already include formatting tokens required model (e.g., special chat tags). additional formatting added function. See 'Prompting Details' section function signature default content customization.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/hf_typicality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality","text":"pair reach min_valid, mean NA; raw invalid strings remain available return_full_responses = TRUE. Cross-product mode (matrix = TRUE) -> list containing: scores: matrix mean typicality scores. raw (return_raw_scores = TRUE): matrix lists, list contains raw numeric scores pair. full_responses (return_full_responses = TRUE): matrix lists, list contains raw text model outputs (error strings) pair. Paired mode (matrix = FALSE) -> tibble columns group, description, mean_score, additionally: raw (return_raw_scores = TRUE): list-column element vector raw numeric scores. full_responses (return_full_responses = TRUE): list-column element character vector raw text model outputs (error strings).","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/hf_typicality.html","id":"get-typicality-ratings-from-hugging-face-models","dir":"Reference","previous_headings":"","what":"Get Typicality Ratings from Hugging Face Models","title":"Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality","text":"hf_typicality() sends structured prompts text-generation model hosted Hugging Face Inference API (self-hosted endpoint) collects numeric ratings (0–100) well description (e.g., adjective) fits group (e.g., occupation). Responses parsed numbers discarded.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/hf_typicality.html","id":"modes","dir":"Reference","previous_headings":"","what":"Modes","title":"Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality","text":"Cross-product (matrix = TRUE, default)    Rate every combination unique groups descriptions. Returns list containing matrices. Paired (matrix = FALSE)                     Rate pairs row--row (length(groups) == length(descriptions)). Returns tibble. pair queried repeatedly least min_valid clean scores obtained retry budget exhausted. One retry block consists n new samples; invalid --range answers silently dropped.","code":""},{"path":"https://jeremie-beucler.github.io/baserater/reference/hf_typicality.html","id":"prompting-details","dir":"Reference","previous_headings":"","what":"Prompting Details","title":"Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality","text":"function constructs final prompt sent model concatenating system_prompt rendered user_prompt_template ({group} {description} substituted actual values), separated two newlines. default system_prompt : default user_prompt_template : responsible ensuring combination prompts, custom prompts, includes specific formatting tokens required model (e.g., instruction tags, chat role indicators like [INST], <|user|>, etc.). function performs concatenation described . Rate-limit friendliness: transient HTTP 429/5xx errors retried (exponential back-), wait_for_model = TRUE set call blocks model ready.","code":"You are expert at accurately reproducing the stereotypical associations humans make, in order to annotate data for experiments. Your focus is to capture common societal perceptions and stereotypes, rather than factual attributes of the groups, even when they are negative or unfounded. Rate how well the description \"{description}\" reflects the prototypical member of the group \"{group}\" on a scale from 0 (\"Not at all\") to 100 (\"Extremely\").  To clarify, consider the following examples: 1. \"Rate how well the description \"FUNNY\" reflects the prototypical member of the group \"CLOWN\" on a scale from 0 (Not at all) to 100 (Extremely).\" A high rating is expected because \"FUNNY\" closely aligns with typical characteristics of a \"CLOWN\". 2. \"Rate how well the description \"FEARFUL\" reflects the prototypical member of the group \"FIREFIGHTER\" on a scale from 0 (Not at all) to 100 (Extremely).\" A low rating is expected because \"FEARFUL\" diverges from typical characteristics of a \"FIREFIGHTER\". 3. \"Rate how well the description \"PATIENT\" reflects the prototypical member of the group \"ENGINEER\" on a scale from 0 (Not at all) to 100 (Extremely).\" A mid-scale rating is expected because \"PATIENT\" neither strongly aligns with nor diverges from typical characteristics of an \"ENGINEER\".  Your response should be a single score between 0 and 100, with no additional text, letters, or symbols."},{"path":"https://jeremie-beucler.github.io/baserater/reference/hf_typicality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality","text":"","code":"if (FALSE) { # \\dontrun{ # --- Minimal reproducible example (toy input) --- toy_groups <- c(\"engineer\", \"clown\", \"firefighter\") toy_descriptions <- c(\"patient\", \"funny\", \"fearful\")  toy_result <- hf_typicality(   groups = toy_groups,   descriptions = toy_descriptions,   model = \"meta-llama/Llama-3-70B-Instruct\",   n = 10,   min_valid = 8,   matrix = FALSE,   return_raw_scores = TRUE,   return_full_responses = FALSE,   verbose = TRUE )  print(toy_result)  # --- Full-scale example using the validation ratings --- # ratings <- download_data(\"validation_ratings\")  # new_scores <- hf_typicality( #   groups                = ratings$group, #   descriptions          = ratings$adjective, #   model                 = \"meta-llama/Llama-3.1-8B-Instruct\", #   n                     = 25, #   min_valid             = 20, #   max_tokens            = 5, #   retries               = 1, #   matrix                = FALSE, #   return_raw_scores     = TRUE, #   return_full_responses = TRUE, #   verbose               = TRUE # )  # head(new_scores) } # }"}]
