---
title: "Introduction to the baserater package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to the baserater package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The `baserater` package allows you to:

- Download a database of group–adjective pairs annotated with stereotype strength scores generated by large language models (GPT-4 and LLaMA 3.3);
- Generate new typicality ratings using a Hugging Face language model of your choice, with customizable prompts and parameters;
- Evaluate newly generated typicality ratings against human ground truth (ratings collected from Prolific participants) and benchmark them against baseline models;
- Automatically construct a new base-rate item database from a group–description typicality matrix.

To learn more about the theoretical framework and validation studies underlying the `baserater` package, see the paper:

**Using Large Language Models to Estimate Belief Strength in Reasoning**
*Jérémie Beucler, Zoe Purcell, Lucie Charles, and Wim De Neys*
(Preprint link — forthcoming)

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  message  = FALSE
)
```

```{r setup}
library(baserater)
library(tidyverse)
```

# 1. Downloading the Data

You can begin by downloading the article datasets using `download_data()`.

Use `download_data()` to retrieve either:
- The full base-rate item database (from GPT-4 and LLaMA 3.3);
- The validation ratings, which include human typicality scores and those generated by GPT-4 and LLaMA 3.3 on 100 group–adjective pairs.
- The two typicality matrices (from GPT-4 and LLaMA 3.3) that were used to generate the base-rate item database.

```{r}
# Load the base-rate database
database <- download_data("database")

# Load the typicality validation ratings
ratings <- download_data("validation_ratings")

# Load the typicality matrices
gpt4_matrix <- download_data("typicality_matrix_gpt4")
llama3_3_matrix <- download_data("typicality_matrix_llama3.3")
```


# 2. Generating New Typicality Ratings from LLMs (Experimental)

You can generate new typicality ratings using a Hugging Face-hosted language model via `hf_typicality()`. Note that this is an experimental feature, as it depends on API availability, and model compatibility.

The function works by sending structured prompts to a large language model and parsing numeric outputs (between 0 and 100) that reflect how well a description (e.g., an adjective) fits a given group. By default, the function uses the same prompt and generation parameters described in the paper.

To ensure reproducibility, we include a precomputed result obtained using the model Llama-3.1-8B-Instruct, a smaller and older version of Llama-3.3-70B-Instruct available on Hugging Face:

```{r}
# Load pre-generated scores
new_scores <- readRDS(system.file("extdata", "new_typicality_scores_llama3.1_8B.rds", package = "baserater"))

head(new_scores)
```

If you’d like to generate new scores yourself, you’ll need:
- a valid Hugging Face API token (use `Sys.setenv(HF_API_TOKEN = "your_token")` or pass directly);
- to accept the terms of use for the model on Hugging Face (when necessary);
- to ensure that your prompt matches the formatting expectations of the model you’re using (e.g., special tags, instruction tokens, etc.).

Here is an example of how the function call might look:

```{r}
# # Original prompt from the paper
# original_system_prompt_content <- "You are expert at accurately reproducing the stereotypical associations humans make, in order to annotate data for experiments. Your focus is to capture common societal perceptions and stereotypes, rather than factual attributes of the groups, even when they are negative or unfounded."
# 
# original_user_prompt_content_template <- 'Rate how well the adjective "{description}" reflects the prototypical member of the group "{group}" on a scale from 0 ("Not at all") to 100 ("Extremely").
# 
# To clarify, consider the following examples:
# 
# 1. "Rate how well the adjective "FUNNY" reflects the prototypical member of the group "CLOWN" on a scale from 0 (Not at all) to 100 (Extremely)." A high rating is expected because the adjective "FUNNY" closely aligns with the typical characteristics of a "CLOWN".
# 
# 2. "Rate how well the adjective "FEARFUL" reflects the prototypical member of the group "FIREFIGHTER" on a scale from 0 (Not at all) to 100 (Extremely)." A low rating is expected because the adjective "FEARFUL" diverges significantly from the typical characteristics of a "FIREFIGHTER".
# 
# 3. "Rate how well the adjective "PATIENT" reflects the prototypical member of the group "ENGINEER" on a scale from 0 (Not at all) to 100 (Extremely)." A mid-scale rating is expected because the adjective "PATIENT" neither closely aligns nor diverges significantly from the typical characteristics of an "ENGINEER".
# 
# Your response should be a single score between 0 and 100, with no additional text, letters, or symbols included.'
# 
# # Llama 3 Formatted Prompts
# # see https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/
# # These special tags are required for models in the LLaMA 3 family
# llama3_system_prompt <- paste0(
#   "<|begin_of_text|>",
#   "<|start_header_id|>system<|end_header_id|>\n\n",
#   original_system_prompt_content,
#   "\n<|eot_id|>"
# )
# 
# llama3_user_prompt_template <- paste0(
#   "<|start_header_id|>user<|end_header_id|>\n\n",
#   original_user_prompt_content_template,
#   "\n<|eot_id|>",
#   "<|start_header_id|>assistant<|end_header_id|>\n\n"
# )

# # Example using the validation ratings
# groups <- ratings$group
# descriptions <- ratings$adjective
# 
# hf_token = "YOUR TOKEN HERE" # default = Sys.getenv("HF_API_TOKEN")
# 
# new_scores <- hf_typicality(
#   groups                = groups,
#   descriptions          = descriptions,
#   model                 = "meta-llama/Llama-3.1-8B-Instruct",
#   hf_token              = hf_token,
#   system_prompt         = llama3_system_prompt,
#   user_prompt_template  = llama3_user_prompt_template,
#   n                     = 3,
#   min_valid             = 2,
#   max_tokens            = 3,
#   retries               = 1,
#   matrix                = FALSE,
#   return_raw_scores     = TRUE,
#   return_full_responses = TRUE,
#   verbose               = TRUE
# )
```

The `hf_typicality()` function supports two modes:
- `matrix = TRUE` (default): Computes a cross-product of unique groups and descriptions. Returns a list with matrices of scores and responses.
- `matrix = FALSE`: Computes row-by-row scores for the group–adjective pairs you supply. Returns a tibble.

See `?hf_typicality` for full documentation and customization options.

# 3. Evaluating New Ratings

You can then assess how well a new model or scoring method captures group–adjective typicality by comparing your ratings to the human ground truth and benchmark models (GPT-4 and LLaMA 3.3).

Suppose you create typicality ratings for the 100 validation items and store them in a data frame with three columns: group, adjective, and rating.

```{r}
# Create a data frame with the same structure as the validation set
new_scores = new_scores %>% 
  mutate(adjective = description,
         rating = mean_score) %>%
  select(group, adjective, rating)

head(new_scores)
```

Use `evaluate_external_ratings()` to compute correlations and display comparisons:

```{r}
# Print correlation summary with human ground truth and baselines
evaluate_external_ratings(new_scores)

# Optionally store the output in a variable
results <- evaluate_external_ratings(new_scores)
print(results)
```

As you can see, the smaller and older Llama-3.1-8B-Instruct does not perform as our baseline models (GPT-4 and LLaMA 3.3-70B).

# 4. Constructing Base-Rate Items from Typicality Matrices

To compute stereotype strength using `extract_base_rate_items()`, you’ll need a typicality matrix—a table of scores where rows correspond to groups and columns correspond to descriptions (e.g., adjectives). Each cell represents how typical a description is for a group.

For example, you can load the GPT-4 typicality matrix using download_data() (the same is available for LLaMA 3.3):

```{r}
#' The typicality matrix from GPT-4 is a data frame with group–adjective pairs and their typicality scores
gpt4_matrix <- download_data("typicality_matrix_gpt4")
head(gpt4_matrix)
```

Extract base-rate items by applying the function:

```{r}
#' Extract base-rate items from the typicality matrix
base_rate_items <- extract_base_rate_items(gpt4_matrix)
```

You can then explore or filter the output, e.g., to view the strongest stereotypes:

```{r}
# View top base-rate items by stereotype strength
base_rate_items %>%
  arrange(desc(StereotypeStrength)) %>%
  head(10)
```

