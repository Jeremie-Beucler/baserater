<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality • baserater</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Experimental Function to Generate Typicality Ratings via Hugging Face — hf_typicality"><meta name="description" content="This function uses the Hugging Face Inference API (or a compatible endpoint)
to generate typicality ratings by querying a large language model (LLM).
Important: This function will only work if:
You have a valid Hugging Face API token (via hf_token or the HF_API_TOKEN environment variable);
You have a valid Hugging Face access token and have accepted the model’s license on the Hub;
The specified model is available and accessible via the Hugging Face API or your own hosted inference endpoint;
The model supports free-text input and generates numeric outputs in response to structured prompts.


Calls to the API are rate-limited, may incur usage costs, and require an internet connection.
This feature is experimental and is not guaranteed to work with all Hugging Face-hosted models."><meta property="og:description" content="This function uses the Hugging Face Inference API (or a compatible endpoint)
to generate typicality ratings by querying a large language model (LLM).
Important: This function will only work if:
You have a valid Hugging Face API token (via hf_token or the HF_API_TOKEN environment variable);
You have a valid Hugging Face access token and have accepted the model’s license on the Hub;
The specified model is available and accessible via the Hugging Face API or your own hosted inference endpoint;
The model supports free-text input and generates numeric outputs in response to structured prompts.


Calls to the API are rate-limited, may incur usage costs, and require an internet connection.
This feature is experimental and is not guaranteed to work with all Hugging Face-hosted models."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">baserater</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/baserater.html">Get started</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/Introduction-to-the-baserater-package.html">Introduction to the baserater package</a></li>
  </ul></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Experimental Function to Generate Typicality Ratings via Hugging Face</h1>

      <div class="d-none name"><code>hf_typicality.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This function uses the Hugging Face Inference API (or a compatible endpoint)
to generate typicality ratings by querying a large language model (LLM).</p>
<p><strong>Important:</strong> This function will only work if:</p><ul><li><p>You have a valid Hugging Face API token (via <code>hf_token</code> or the <code>HF_API_TOKEN</code> environment variable);</p></li>
<li><p>You have a valid Hugging Face access token and have accepted the model’s license on the Hub;</p></li>
<li><p>The specified model is available and accessible via the Hugging Face API or your own hosted inference endpoint;</p></li>
<li><p>The model supports free-text input and generates numeric outputs in response to structured prompts.</p></li>
</ul><p>Calls to the API are rate-limited, may incur usage costs, and require an internet connection.
This feature is <strong>experimental</strong> and is not guaranteed to work with all Hugging Face-hosted models.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">hf_typicality</span><span class="op">(</span></span>
<span>  <span class="va">groups</span>,</span>
<span>  <span class="va">descriptions</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"meta-llama/Llama-3-70B-Instruct"</span>,</span>
<span>  custom_url <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  hf_token <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"HF_API_TOKEN"</span><span class="op">)</span>,</span>
<span>  n <span class="op">=</span> <span class="fl">25</span>,</span>
<span>  min_valid <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">ceiling</a></span><span class="op">(</span><span class="fl">0.8</span> <span class="op">*</span> <span class="va">n</span><span class="op">)</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  top_p <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  max_tokens <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  retries <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  matrix <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  return_raw_scores <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  return_full_responses <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  verbose <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/interactive.html" class="external-link">interactive</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  system_prompt <span class="op">=</span></span>
<span>    <span class="st">"You are expert at accurately reproducing the stereotypical associations humans make, in order to annotate data for experiments.\nYour focus is to capture common societal perceptions and stereotypes, rather than factual attributes of the groups, even when they are negative or unfounded."</span>,</span>
<span>  user_prompt_template <span class="op">=</span></span>
<span>    <span class="st">"Rate how well the description \"{description}\" reflects the prototypical member of the group \"{group}\" on a scale from 0 (\"Not at all\") to 100 (\"Extremely\").\n\nTo clarify, consider the following examples:\n1. \"Rate how well the description \"FUNNY\" reflects the prototypical member of the group \"CLOWN\" on a scale from 0 (Not at all) to 100 (Extremely).\" A high rating is expected because \"FUNNY\" closely aligns with typical characteristics of a \"CLOWN\".\n2. \"Rate how well the description \"FEARFUL\" reflects the prototypical member of the group \"FIREFIGHTER\" on a scale from 0 (Not at all) to 100 (Extremely).\" A low rating is expected because \"FEARFUL\" diverges from typical characteristics of a \"FIREFIGHTER\".\n3. \"Rate how well the description \"PATIENT\" reflects the prototypical member of the group \"ENGINEER\" on a scale from 0 (Not at all) to 100 (Extremely).\" A mid-scale rating is expected because \"PATIENT\" neither strongly aligns with nor diverges from typical characteristics of an \"ENGINEER\".\n\nYour response should be a single score between 0 and 100, with no additional text, letters, or symbols."</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-groups-descriptions">groups, descriptions<a class="anchor" aria-label="anchor" href="#arg-groups-descriptions"></a></dt>
<dd><p>Character vectors. <em>When</em> <code>matrix = FALSE</code> they
<strong>must</strong> be the same length.</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>Model ID on Hugging Face (ignored if <code>custom_url</code> is
supplied).</p></dd>


<dt id="arg-custom-url">custom_url<a class="anchor" aria-label="anchor" href="#arg-custom-url"></a></dt>
<dd><p>Fully-qualified HTTPS URL of a private Inference Endpoint
or self-hosted TGI server.</p></dd>


<dt id="arg-hf-token">hf_token<a class="anchor" aria-label="anchor" href="#arg-hf-token"></a></dt>
<dd><p>API token (defaults to <code>Sys.getenv("HF_API_TOKEN")</code>).</p></dd>


<dt id="arg-n">n<a class="anchor" aria-label="anchor" href="#arg-n"></a></dt>
<dd><p>Samples requested per retry block (&gt;= 1).</p></dd>


<dt id="arg-min-valid">min_valid<a class="anchor" aria-label="anchor" href="#arg-min-valid"></a></dt>
<dd><p>Minimum numeric scores required per pair.</p></dd>


<dt id="arg-temperature-top-p-max-tokens">temperature, top_p, max_tokens<a class="anchor" aria-label="anchor" href="#arg-temperature-top-p-max-tokens"></a></dt>
<dd><p>Generation controls.</p></dd>


<dt id="arg-retries">retries<a class="anchor" aria-label="anchor" href="#arg-retries"></a></dt>
<dd><p>Maximum number of <em>additional</em> retry blocks.</p></dd>


<dt id="arg-matrix">matrix<a class="anchor" aria-label="anchor" href="#arg-matrix"></a></dt>
<dd><p><code>TRUE</code> = cross-product, <code>FALSE</code> = paired.</p></dd>


<dt id="arg-return-raw-scores">return_raw_scores<a class="anchor" aria-label="anchor" href="#arg-return-raw-scores"></a></dt>
<dd><p>If <code>TRUE</code>, also returns the vector(s) of raw valid numeric scores.</p></dd>


<dt id="arg-return-full-responses">return_full_responses<a class="anchor" aria-label="anchor" href="#arg-return-full-responses"></a></dt>
<dd><p>If <code>TRUE</code>, also returns all raw text model outputs
(or error strings from failed attempts) for each query.</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>If <code>TRUE</code>, prints progress: pair labels, retry counts,
running tallies, and raw model responses/errors as they occur.</p></dd>


<dt id="arg-system-prompt">system_prompt<a class="anchor" aria-label="anchor" href="#arg-system-prompt"></a></dt>
<dd><p>Prompt string for the system message. See the 'Prompting Details' section and function signature for default content and customization.</p></dd>


<dt id="arg-user-prompt-template">user_prompt_template<a class="anchor" aria-label="anchor" href="#arg-user-prompt-template"></a></dt>
<dd><p>Prompt template for the user message, with <code>{group}</code> and <code>{description}</code> placeholders. The prompt should already include any formatting tokens required by your model (e.g., special chat tags). No additional formatting is added by the function. See the 'Prompting Details' section and function signature for default content and customization.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>If a pair cannot reach min_valid, its mean is NA; raw invalid strings remain available when return_full_responses = TRUE.
Cross-product mode (<code>matrix = TRUE</code>) -&gt; a list containing:</p><ul><li><p><code>scores</code>: A matrix of mean typicality scores.</p></li>
<li><p><code>raw</code> (if <code>return_raw_scores = TRUE</code>): A matrix of lists, where each list contains the raw numeric scores for that pair.</p></li>
<li><p><code>full_responses</code> (if <code>return_full_responses = TRUE</code>): A matrix of lists, where each list contains all raw text model outputs (or error strings) for that pair.</p></li>
</ul><p>Paired mode (<code>matrix = FALSE</code>) -&gt; a tibble with columns for <code>group</code>, <code>description</code>, <code>mean_score</code>, and additionally:</p><ul><li><p><code>raw</code> (if <code>return_raw_scores = TRUE</code>): A list-column where each element is a vector of raw numeric scores.</p></li>
<li><p><code>full_responses</code> (if <code>return_full_responses = TRUE</code>): A list-column where each element is a character vector of all raw text model outputs (or error strings).</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="get-typicality-ratings-from-hugging-face-models">Get Typicality Ratings from Hugging Face Models<a class="anchor" aria-label="anchor" href="#get-typicality-ratings-from-hugging-face-models"></a></h2>
    <p><strong>hf_typicality()</strong> sends structured prompts to any text-generation model
hosted on the Hugging Face Inference API (or a self-hosted endpoint) and
collects <em>numeric</em> ratings (0–100) of how well a <em>description</em> (e.g., an
adjective) fits a <em>group</em> (e.g., an occupation). Responses that cannot be
parsed into numbers are discarded.</p><div class="section">
<h3 id="modes">Modes<a class="anchor" aria-label="anchor" href="#modes"></a></h3>

<ul><li><p><strong>Cross-product</strong> (<code>matrix = TRUE</code>, <em>default</em>)    Rate every combination of
the <em>unique</em> <code>groups</code> and <code>descriptions</code>. Returns a list containing matrices.</p></li>
<li><p><strong>Paired</strong> (<code>matrix = FALSE</code>)                     Rate the pairs row-by-row
(<code>length(groups) == length(descriptions)</code>). Returns a tibble.</p></li>
</ul><p>Each pair is queried repeatedly until at least <strong><code>min_valid</code></strong> clean scores
are obtained or the retry budget is exhausted. One <em>retry block</em> consists of
<strong><code>n</code></strong> new samples; invalid or out-of-range answers are silently dropped.</p>
</div>

    </div>
    <div class="section level2">
    <h2 id="prompting-details">Prompting Details<a class="anchor" aria-label="anchor" href="#prompting-details"></a></h2>


<p>The function constructs the final prompt sent to the model by concatenating the <code>system_prompt</code>
and the rendered <code>user_prompt_template</code> (where <code>{group}</code> and <code>{description}</code>
are substituted with the actual values), separated by two newlines.</p>
<p>The default <code>system_prompt</code> is:</p><div class="sourceCode"><pre><code>
You are expert at accurately reproducing the stereotypical associations humans make, in order to annotate data for experiments.
Your focus is to capture common societal perceptions and stereotypes, rather than factual attributes of the groups, even when they are negative or unfounded.
</code></pre></div><p>The default <code>user_prompt_template</code> is:</p><div class="sourceCode"><pre><code>
Rate how well the description "{description}" reflects the prototypical member of the group "{group}" on a scale from 0 ("Not at all") to 100 ("Extremely").

To clarify, consider the following examples:
1. "Rate how well the description "FUNNY" reflects the prototypical member of the group "CLOWN" on a scale from 0 (Not at all) to 100 (Extremely)." A high rating is expected because "FUNNY" closely aligns with typical characteristics of a "CLOWN".
2. "Rate how well the description "FEARFUL" reflects the prototypical member of the group "FIREFIGHTER" on a scale from 0 (Not at all) to 100 (Extremely)." A low rating is expected because "FEARFUL" diverges from typical characteristics of a "FIREFIGHTER".
3. "Rate how well the description "PATIENT" reflects the prototypical member of the group "ENGINEER" on a scale from 0 (Not at all) to 100 (Extremely)." A mid-scale rating is expected because "PATIENT" neither strongly aligns with nor diverges from typical characteristics of an "ENGINEER".

Your response should be a single score between 0 and 100, with no additional text, letters, or symbols.
</code></pre></div><p>You are responsible for ensuring that the combination of these prompts, or your custom prompts,
includes any specific formatting tokens required by your model (e.g., instruction tags,
chat role indicators like <code>[INST]</code>, <code>&lt;|user|&gt;</code>, etc.). The function itself
only performs the concatenation described above.</p>
<p>Rate-limit friendliness: transient HTTP 429/5xx errors are retried (exponential
back-off), and <code>wait_for_model = TRUE</code> is set so the call blocks until the
model is ready.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># --- Minimal reproducible example (toy input) ---</span></span></span>
<span class="r-in"><span><span class="va">toy_groups</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"engineer"</span>, <span class="st">"clown"</span>, <span class="st">"firefighter"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">toy_descriptions</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"patient"</span>, <span class="st">"funny"</span>, <span class="st">"fearful"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">toy_result</span> <span class="op">&lt;-</span> <span class="fu">hf_typicality</span><span class="op">(</span></span></span>
<span class="r-in"><span>  groups <span class="op">=</span> <span class="va">toy_groups</span>,</span></span>
<span class="r-in"><span>  descriptions <span class="op">=</span> <span class="va">toy_descriptions</span>,</span></span>
<span class="r-in"><span>  model <span class="op">=</span> <span class="st">"meta-llama/Llama-3-70B-Instruct"</span>,</span></span>
<span class="r-in"><span>  n <span class="op">=</span> <span class="fl">10</span>,</span></span>
<span class="r-in"><span>  min_valid <span class="op">=</span> <span class="fl">8</span>,</span></span>
<span class="r-in"><span>  matrix <span class="op">=</span> <span class="cn">FALSE</span>,</span></span>
<span class="r-in"><span>  return_raw_scores <span class="op">=</span> <span class="cn">TRUE</span>,</span></span>
<span class="r-in"><span>  return_full_responses <span class="op">=</span> <span class="cn">FALSE</span>,</span></span>
<span class="r-in"><span>  verbose <span class="op">=</span> <span class="cn">TRUE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">toy_result</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># --- Full-scale example using the validation ratings ---</span></span></span>
<span class="r-in"><span><span class="co"># ratings &lt;- download_data("validation_ratings")</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># new_scores &lt;- hf_typicality(</span></span></span>
<span class="r-in"><span><span class="co">#   groups                = ratings$group,</span></span></span>
<span class="r-in"><span><span class="co">#   descriptions          = ratings$adjective,</span></span></span>
<span class="r-in"><span><span class="co">#   model                 = "meta-llama/Llama-3.1-8B-Instruct",</span></span></span>
<span class="r-in"><span><span class="co">#   n                     = 25,</span></span></span>
<span class="r-in"><span><span class="co">#   min_valid             = 20,</span></span></span>
<span class="r-in"><span><span class="co">#   max_tokens            = 5,</span></span></span>
<span class="r-in"><span><span class="co">#   retries               = 1,</span></span></span>
<span class="r-in"><span><span class="co">#   matrix                = FALSE,</span></span></span>
<span class="r-in"><span><span class="co">#   return_raw_scores     = TRUE,</span></span></span>
<span class="r-in"><span><span class="co">#   return_full_responses = TRUE,</span></span></span>
<span class="r-in"><span><span class="co">#   verbose               = TRUE</span></span></span>
<span class="r-in"><span><span class="co"># )</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># head(new_scores)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Jeremie Beucler.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.2.</p>
</div>

    </footer></div>





  </body></html>

