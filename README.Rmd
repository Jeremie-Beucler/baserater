---
title: "Getting started with the baserater package"
output: github_document
vignette: >
  %\VignetteIndexEntry{Getting started with the baserater package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The `baserater` package allows to:  

- Download LLM‑generated base-rate item datasets and human validation ratings from the original paper.  
- Generate new typicality scores with any Hugging Face model.  
- Benchmark new scores against human ground truth, and compare performance against strong LLM baselines.  
- Build base‑rate items database from typicality matrices.

It is designed to streamline the creation of base-rate neglect items for reasoning experiments. You can use it to generate new typicality ratings with any LLM on the Hugging Face platform using your own prompts and parameters, benchmark these ratings against human data, and construct a base-rate item dataset from the generated scores.

To learn more about the theoretical framework and studies underlying the baserater package, see the paper: *Using Large Language Models to Estimate Belief Strength in Reasoning* (Beucler et al., forthcoming).

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  message  = FALSE
)
```

## Installation

```{r}
# development version
# install.packages("pak")
pak::pak("Jeremie-Beucler/baserater")
```

## Download data

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(baserater)

database <- download_data("database")             # full base‑rate item database
ratings   <- download_data("validation_ratings")  # 100 human‑rated items
```

## Generate scores with an LLM

```{r}
new_scores <- hf_typicality(
  groups       = c("nurse", "clown"),
  descriptions = c("caring", "funny"),
  model        = "meta-llama/Llama-3.1-8B-Instruct",
  hf_token     = "your_token_here",
  n            = 3,
  min_valid    = 2,
  matrix       = FALSE
)
```

## Evaluate model predictions

```{r, message=FALSE, warning = FALSE, echo=FALSE}
# load new precomputed new scores
new_scores <- readRDS(system.file("extdata", "new_typicality_scores_llama3.1_8B.rds", package = "baserater"))

new_scores <- new_scores %>% 
  mutate(adjective = description,
         rating = mean_score) %>% select(group, adjective, rating)
```


```{r}
evaluate_external_ratings(new_scores)
```

## Build a base-rate item dataset

```{r}
gpt4_matrix    <- download_data("typicality_matrix_gpt4")
base_rate_tbl  <- extract_base_rate_items(gpt4_matrix)
```

## More

Full documentation: https://jeremie-beucler.github.io/baserater/

Original paper: *Using Large Language Models to Estimate Belief Strength in Reasoning* (Beucler et al., forthcoming).

## License

GPL-3
