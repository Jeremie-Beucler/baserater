---
output: github_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# baserater

`baserater` helps you:
- Download LLM‑generated stereotype datasets and human validation ratings (from the accompanying paper)
- Generate new typicality scores with any Hugging Face model
- Benchmark scores against human ground truth and strong LLM baselines
- Build base‑rate stereotype tables from typicality matrices

## Installation

```{r}
# for dev version
# install.packages("pak")
pak::pak("Jeremie-Beucler/baserater")
```

## Download data

```{r}
library(baserater)

database <- download_data("database")             # full base‑rate set
ratings   <- download_data("validation_ratings")  # 100 human‑rated items
```

## Evaluate model predictions

```{r}
# Compare model ratings to human judgments and baseline LLMs (GPT-4, LLaMA 3.3)
evaluate_external_ratings(ratings)  # Replace with your own scores if needed
```

## Generate scores with an LLM

```{r}
new_scores <- hf_typicality(
  groups       = c("nurse", "clown"),
  descriptions = c("caring", "funny"),
  model        = "meta-llama/Llama-3.1-8B-Instruct",
  hf_token     = "your_token_here",
  n            = 10,
  min_valid    = 8,
  matrix       = FALSE # matrix = FALSE → pairwise; TRUE → full group–description matrix
)
```

## Build a base‑rate dataset

```{r}
gpt4_matrix    <- download_data("similarity_matrix_gpt4")
# create base-rate items with stereotype strength for each pair of groups and description
base_rate_tbl  <- extract_base_rate_items(gpt4_matrix)
```

## More

Full documentation: https://jeremie-beucler.github.io/baserater/

Paper: Using Large Language Models to Estimate Belief Strength in Reasoning (Beucler et al. - forthcoming)

## License

GPL‑3
